{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import BatchNormalization\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from numpy.random import seed\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import normaltest\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate the middle point use stochastic interpolation built by last group\n",
    "def interpolate_gaussian(data, num_between, var=8):  # Data should be univariate time series in pd dataframe\n",
    "\n",
    "    new_dat = np.zeros(1)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        new_dat = np.append(new_dat, data.iloc[i, :].values[0])\n",
    "        if (i != (len(data) - 1)):\n",
    "            step = (data.iloc[i + 1, :].values[0] - data.iloc[i, :].values[0]) / (num_between + 1)\n",
    "            for j in range(num_between):\n",
    "                k = data.iloc[i, :].values[0] + j * step + np.random.normal(loc=0.0,\n",
    "                                                                            scale=1 / (var * (num_between + 1)))\n",
    "                new_dat = np.append(new_dat, k)\n",
    "    interpolated_data = pd.DataFrame(new_dat)\n",
    "    interpolated_data = interpolated_data.iloc[1:len(interpolated_data), :]\n",
    "\n",
    "    return (interpolated_data)\n",
    "\n",
    "\n",
    "def prep_dat(d, n_step=270, n_ahead=360):  # Train a network for every step ahead (4 quarters, 8 quarters)\n",
    "    # Prepare data for input/output of NNs, n_step is number of historical time steps (days) to include in regression task\n",
    "    # to predict next quarter cash flows, assuming 90 days in a quarter\n",
    "\n",
    "    a = d.copy()\n",
    "\n",
    "    for i in range(1, n_step):\n",
    "        ind = \"t+\" + str(i)\n",
    "        a[ind] = a.iloc[:, 0].shift(-i)  # Use n_step historical observations to predict next quarter\n",
    "\n",
    "    a[\"Q_Next\"] = a.iloc[:, 0].shift(-(n_step + n_ahead))  # Next quarter(s) prediction\n",
    "\n",
    "    a.dropna(\n",
    "        inplace=True)  # last (n_step + n_ahead) rows will be NANs since shifting forward for \"Next Quarter\" Variable\n",
    "\n",
    "    return (a)\n",
    "\n",
    "#get distribution\n",
    "def get_main(files, sheet):\n",
    "    d = pd.read_excel(files, sheet_name = sheet, header = 2, usecols = \"A:G\",index_col = 0)[['Distributed']]\n",
    "    d.dropna()\n",
    "    delta = data_process(d)\n",
    "    delta = delta.rename(columns = {0:sheet})\n",
    "    return delta\n",
    "\n",
    "\n",
    "def data_process(data):\n",
    "\n",
    "    log_data = np.log(data).diff().dropna()\n",
    "    _train = interpolate_gaussian(log_data.iloc[0:math.floor(0.7*len(log_data))],in_between,6)\n",
    "    _test = interpolate_gaussian(log_data.iloc[math.floor(0.7*len(log_data)):len(log_data)],in_between,8)\n",
    "    result = pd.concat([_train, _test])\n",
    "    return result \n",
    "\n",
    "def get_macro(files, sheet):\n",
    "    d = pd.read_excel(files, sheet_name = sheet, header = 2, usecols = \"A:G\",index_col = 0)[[\"Called Up\"]]\n",
    "    d = d.dropna()\n",
    "    delta = data_process(d)\n",
    "    delta = delta.rename(columns = {0:sheet})\n",
    "    return delta\n",
    "\n",
    "# take pd dataframe into processable numpy array\n",
    "def trans_dat(data):\n",
    "    pre_col = 0\n",
    "    for col in data.columns:\n",
    "        if pre_col == 0:\n",
    "            sup_dat = prep_dat(data[[col]]).drop(columns='Q_Next').values\n",
    "            sup_dat = np.expand_dims(sup_dat,axis=2)\n",
    "            pre_col = col\n",
    "        else:\n",
    "            \n",
    "            sup_dat1 = prep_dat(data[[col]]).drop(columns='Q_Next').values\n",
    "            sup_dat1 = np.expand_dims(sup_dat1,axis=2)\n",
    "            sup_dat = np.concatenate((sup_dat,sup_dat1),axis=2)            \n",
    "            \n",
    "    return sup_dat\n",
    "\n",
    "def split_dataset_x(data):\n",
    "    # split into standard weeks\n",
    "    num_features = data.shape[2]\n",
    "    train =  data[0:math.floor(0.7 * len(data)),0:hidden_size,0:num_features]\n",
    "    test = data[math.floor(0.7 * len(data)):-1,0:hidden_size,0:num_features]\n",
    "#     # restructure into windows of weekly data\n",
    "#     train = array(split(train, int(len(train)/7)))\n",
    "#     test = array(split(test, int(len(test)/7)))\n",
    "    return train, test\n",
    "# 1176/7 = 168, 546/7 = 78\n",
    "def split_dataset_y(data):\n",
    "    train = np.expand_dims(data[0:math.floor(0.7 * len(data))],axis = 1)\n",
    "    test = np.expand_dims(data[math.floor(0.7 * len(data)):-1],axis = 1)\n",
    "    return train,test\n",
    "\n",
    "\n",
    "def evaluation_model(model):\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    history = model.history\n",
    "    \n",
    "\n",
    "    y_hat_train = model.predict(X_train,verbose = 0)\n",
    "    y_hat_test = model.predict(X_test,verbose = 0)\n",
    "    \n",
    "    train_mse = mean_squared_error(y_hat_train[:,0],y_train)\n",
    "    test_mse = mean_squared_error(y_hat_test[:,0],y_test)\n",
    "    \n",
    "    print(\"Train set MSE is\",train_mse)\n",
    "    print(\"Test set MSE is\",test_mse)\n",
    "    \n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.plot(history.history['loss'],label = 'training loss')\n",
    "    plt.plot(history.history['val_loss'],label = 'validation loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.title('Training loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.plot(y_hat_train[:,0],label = 'prediction')\n",
    "    plt.plot(y_train,label = 'actual')\n",
    "    plt.ylabel('value')\n",
    "    plt.xlabel('data point')\n",
    "    plt.title('Train set actual/predict')\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.plot(y_hat_test[:,0],label = 'prediction')\n",
    "    plt.plot(y_test,label = 'actual')\n",
    "    plt.ylabel('value')\n",
    "    plt.xlabel('data point')\n",
    "    plt.suptitle('Test set actual/predict')\n",
    "    plt.legend()\n",
    "    \n",
    "\n",
    "metric = 'Called Up'\n",
    "in_between = 90\n",
    "#The time used to predict the future, equal to n_step in prep_dat\n",
    "hidden_size = 270"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:48: RuntimeWarning: divide by zero encountered in log\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "#main data\n",
    "filename = \"VC_Funds_Stats_2014.xlsx\"\n",
    "dat_all = get_macro(filename,\"All\")\n",
    "\n",
    "\n",
    "#macro factors\n",
    "filename = \"Copy of Buyout_Funds_Stats_2014.xlsx\"\n",
    "dat_ISMmanu = get_macro(filename, \"ISMmanu\")\n",
    "dat_GDP = get_macro(filename, \"GDP\")\n",
    "dat_unemployment = get_macro(filename, \"unemployment\")\n",
    "ppi = get_macro(filename,\"PPI\")\n",
    "ISMnonmanu = get_macro(filename,\"ISMnon-manu\")\n",
    "nfp = get_macro(filename,\"NFP\")\n",
    "ppi_m = get_macro(filename,\"core PPI m_m\")\n",
    "ppi_y = get_macro(filename,\"core PPI year\")\n",
    "retail = get_macro(filename,\"retail index\")\n",
    "core_retail = get_macro(filename,\"core retail index\")\n",
    "consumer_m = get_macro(filename,\"consumer product index m_m\")\n",
    "cpi_m = get_macro(filename,\"core CPI m_m\")\n",
    "cpi_y = get_macro(filename,\"core CPI year\")\n",
    "new_house = get_macro(filename,\"New Housing\")\n",
    "consumer_sentiment = get_macro(filename,\"Michigan Consumer Sentiment Ind\")\n",
    "sale_house = get_macro(filename,\"House sales\")\n",
    "dgo = get_macro(filename,\"Durable Goods Orders m_m\")\n",
    "dgo_et = get_macro(filename,\"DGO m_m exclude transportation\")\n",
    "CCI = get_macro(filename,\"Conference Board's CCI\")\n",
    "ir = get_macro(filename,\"interest rate\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Univariate input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dat_all\n",
    "X = trans_dat(dataset)\n",
    "y = prep_dat(dat_all)['Q_Next']\n",
    "X_train, X_test = split_dataset_x(X)\n",
    "y_train, y_test = split_dataset_y(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run again if meet exploding gradient problem and prediction will have NaN value\n",
    "\n",
    "def build_model(train_x,train_y,n_input):\n",
    "    # prepare data\n",
    "#     train_x, train_y = to_supervised(train, n_input)\n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 0, 100, 100\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # reshape output into [samples, timesteps, features]\n",
    "    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1],1))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "    model.add(RepeatVector(n_outputs))\n",
    "    model.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "#     model.add((Dense(200,  kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01),activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.1))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    opt = tf.keras.optimizers.Adam()\n",
    "    model.compile(loss='mse', optimizer=opt)\n",
    "    # fit network\n",
    "    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose,validation_split=0.2)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(X_train,y_train,270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a05444a22b39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluation_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-c4908fb31266>\u001b[0m in \u001b[0;36mevaluation_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0my_hat_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mtrain_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0mtest_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \"\"\"\n\u001b[1;32m    240\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 241\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \"\"\"\n\u001b[1;32m     77\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 542\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "evaluation_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19 Macro factors with LSTM & encoder-decoder LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3  = pd.concat([dat_all, dat_unemployment, dat_ISMmanu, dat_GDP, ppi,\n",
    "                     ISMnonmanu, nfp, ppi_m, ppi_y, retail, core_retail, consumer_m,\n",
    "                       cpi_y, new_house, consumer_sentiment, \n",
    "                      sale_house, dgo, dgo_et, CCI, ir\n",
    "                    ],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,16))\n",
    "sn.heatmap(dataset3.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trans_dat(dataset3)\n",
    "y = prep_dat(dat_all)['Q_Next']\n",
    "X_train, X_test = split_dataset_x(X)\n",
    "y_train, y_test = split_dataset_y(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_encoder(train_x,train_y,n_input):\n",
    "    # prepare data\n",
    "\n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 0, 100, 100\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # reshape output into [samples, timesteps, features]\n",
    "    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "    model.add(RepeatVector(n_outputs))\n",
    "    model.add((Dense(200,activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    # fit network\n",
    "    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose,validation_split=0.2)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def build_model(train_x,train_y,n_input):\n",
    "    # prepare data\n",
    "#     train_x, train_y = to_supervised(train, n_input)\n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 0, 100, 100\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # reshape output into [samples, timesteps, features]\n",
    "    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1]))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "#     model.add(RepeatVector(n_outputs))\n",
    "#     model.add(LSTM(100, activation='tanh', return_sequences=True))\n",
    "#     model.add((Dense(200,  kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01),activation='relu')))\n",
    "    model.add((Dense(200, activation='relu')))\n",
    "#     model.add(Dropout(0.1))\n",
    "    model.add((Dense(n_outputs)))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    # fit network\n",
    "    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose,validation_split=0.2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(X_train,y_train,270)\n",
    "model_encoder = build_model_encoder(X_train,y_train,270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_model(model_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  feature selection on the macro factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trans_dat(dataset3)\n",
    "y = prep_dat(dat_all)['Q_Next']\n",
    "X_train, X_test = split_dataset_x(X)\n",
    "y_train, y_test = split_dataset_y(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy = dataset3['All']\n",
    "testx = dataset3.iloc[:,1:]\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "rfe_selector = RFE(estimator=LinearRegression(), n_features_to_select=9, step=10, verbose=5)\n",
    "rfe_selector.fit(testx, testy)\n",
    "rfe_support = rfe_selector.get_support()\n",
    "rfe_feature = testx.loc[:,rfe_support].columns.tolist()\n",
    "print(str(len(rfe_feature)), 'selected features',rfe_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=7)\n",
    "fit = pca.fit(testx)\n",
    "# summarize components\n",
    "print(\"Explained Variance: %s\" % fit.explained_variance_ratio_)\n",
    "print(sum(fit.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = dataset3[['All']+rfe_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset2 = pd.concat([dat_all, dat_unemployment, dat_ISMmanu, dat_GDP,ppi],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trans_dat(dataset2)\n",
    "y = prep_dat(dat_all)['Q_Next']\n",
    "X_train, X_test = split_dataset_x(X)\n",
    "y_train, y_test = split_dataset_y(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(X_train,y_train,270)\n",
    "model_encoder = build_model_encoder(X_train,y_train,270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_model(model_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN encoder-decoder LSTM univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dat_all\n",
    "X = trans_dat(dataset)\n",
    "y = prep_dat(dat_all)['Q_Next']\n",
    "X_train, X_test = split_dataset_x(X)\n",
    "y_train, y_test = split_dataset_y(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_conv(train_x,train_y,n_input):\n",
    "    # prepare data\n",
    "#     train_x, train_y = to_supervised(train, n_input)\n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 0, 100, 100\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # reshape output into [samples, timesteps, features]\n",
    "    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(RepeatVector(n_outputs))\n",
    "    model.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    \n",
    "    \n",
    "    # fit network\n",
    "    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose,validation_split=0.2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model_conv(X_train,y_train,270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN encoder-decoder LSTM multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_conv1(train_x,train_y,n_input):\n",
    "    # prepare data\n",
    "#     train_x, train_y = to_supervised(train, n_input)\n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 0, 100, 100\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # reshape output into [samples, timesteps, features]\n",
    "    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "    # define model\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(RepeatVector(n_outputs))\n",
    "    \n",
    "    model.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "    model.add(RepeatVector(n_outputs))\n",
    "    model.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    \n",
    "    \n",
    "    # fit network\n",
    "    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose,validation_split=0.2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trans_dat(dataset3)\n",
    "y = prep_dat(dat_all)['Q_Next']\n",
    "X_train, X_test = split_dataset_x(X)\n",
    "y_train, y_test = split_dataset_y(y)\n",
    "print(X_train.shape,'\\n',y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model_conv1(X_train,y_train,270)\n",
    "evaluation_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv 2-d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike an LSTM that reads the data in directly in order to calculate internal state and state transitions, and unlike the CNN-LSTM that is interpreting the output from CNN models, the ConvLSTM is using convolutions directly as part of reading input into the LSTM units themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dat(d, n_step=270, n_ahead=360):  # Train a network for every step ahead (4 quarters, 8 quarters)\n",
    "    # Prepare data for input/output of NNs, n_step is number of historical time steps (days) to include in regression task\n",
    "    # to predict next quarter cash flows, assuming 90 days in a quarter\n",
    "\n",
    "    a = d.copy()\n",
    "\n",
    "    for i in range(1, n_step):\n",
    "        ind = \"t+\" + str(i)\n",
    "        a[ind] = a.iloc[:, 0].shift(-i)  # Use n_step historical observations to predict next quarter\n",
    "\n",
    "    a[\"Q_Next\"] = a.iloc[:, 0].shift(-(n_step + n_ahead))  # Next quarter(s) prediction\n",
    "\n",
    "    a.dropna(\n",
    "        inplace=True)  # last (n_step + n_ahead) rows will be NANs since shifting forward for \"Next Quarter\" Variable\n",
    "\n",
    "    return (a)\n",
    "\n",
    "\n",
    "def evaluation_model_conv(model,X_train,X_test):\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    history = model.history\n",
    "    \n",
    "    n_timesteps, n_features = X_train.shape[1], X_train.shape[2]\n",
    "    X_train = X_train.reshape((X_train.shape[0], n_steps, 1, n_length, n_features))\n",
    "    X_test = X_test.reshape((X_test.shape[0], n_steps, 1, n_length, n_features))\n",
    "    y_hat_train = model.predict(X_train,verbose = 0)\n",
    "    y_hat_test = model.predict(X_test,verbose = 0)\n",
    "    \n",
    "    train_mse = mean_squared_error(y_hat_train[:,0],y_train)\n",
    "    test_mse = mean_squared_error(y_hat_test[:,0],y_test)\n",
    "    \n",
    "    print(\"Train set MSE is\",train_mse)\n",
    "    print(\"Test set MSE is\",test_mse)\n",
    "    \n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.plot(history.history['loss'],label = 'training loss')\n",
    "    plt.plot(history.history['val_loss'],label = 'validation loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.title('Training loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.plot(y_hat_train[:,0],label = 'prediction')\n",
    "    plt.plot(y_train,label = 'actual')\n",
    "    plt.ylabel('value')\n",
    "    plt.xlabel('data point')\n",
    "    plt.title('Train set actual/predict')\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.plot(y_hat_test[:,0],label = 'prediction')\n",
    "    plt.plot(y_test,label = 'actual')\n",
    "    plt.ylabel('value')\n",
    "    plt.xlabel('data point')\n",
    "    plt.suptitle('Test set actual/predict')\n",
    "    plt.legend()\n",
    "\n",
    "#The time used to predict the future, equal to n_step in prep_dat\n",
    "hidden_size = 270\n",
    "n_steps, n_length = 90,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trans_dat(dataset3)\n",
    "y = prep_dat(dat_all)['Q_Next']\n",
    "X_train, X_test = split_dataset_x(X)\n",
    "y_train, y_test = split_dataset_y(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_conv2(train_x,train_y,n_input):\n",
    "    # prepare data\n",
    "#     train_x, train_y = to_supervised(train, n_input)\n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 0, 100, 100\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # reshape output into [samples, timesteps, features]\n",
    "    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "    \n",
    "    # reshape into subsequences [samples, time steps, rows, cols, channels]\n",
    "    train_x = train_x.reshape((train_x.shape[0], n_steps, 1, n_length, n_features))\n",
    "    # define model\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, n_features)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(RepeatVector(n_outputs))\n",
    "    \n",
    "#     model.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "#     model.add(RepeatVector(n_outputs))\n",
    "    model.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    \n",
    "    \n",
    "    # fit network\n",
    "    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose,validation_split=0.25)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model_conv2(X_train,y_train,270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluation_model_conv(model,X_train,X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use autoencoder to compress the data after interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lstm autoencoder recreate sequence\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils import plot_model\n",
    "# define input sequence\n",
    "\n",
    "sequence = dat_all.values\n",
    "n_in = len(sequence)\n",
    "sequence = sequence.reshape((1, n_in, 1))\n",
    "# reshape input into [samples, timesteps, features]\n",
    "def amodel1(sequence):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_in,1)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(RepeatVector(n_in))\n",
    "    model.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    # define model\n",
    "    # model = Sequential()\n",
    "    # model.add(LSTM(100, activation='relu', input_shape=(n_in,1)))\n",
    "    # model.add(RepeatVector(n_in))\n",
    "    # model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "    # model.add(TimeDistributed(Dense(1)))\n",
    "    # model.compile(optimizer='adam', loss='mse')\n",
    "    # fit model\n",
    "    model.fit(sequence, sequence, epochs=100, verbose=0)\n",
    "    # demonstrate recreation\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def amodel2(sequence):\n",
    "\n",
    "\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_shape=(n_in,1)))\n",
    "#     model.add(RepeatVector(n_in))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    # fit model\n",
    "    model.fit(sequence, sequence, epochs=100, verbose=0)\n",
    "    # demonstrate recreation\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = amodel2(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model2.predict(sequence)\n",
    "mean_squared_error(yhat[0,:,0],sequence[0,:,0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(yhat[0,:,0],label = 'predict')\n",
    "plt.plot(sequence[0,:,0], label = 'true')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
